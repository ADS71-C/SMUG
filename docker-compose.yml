version: "3"
services:
  rabbitmq:
    image: byteflair/rabbitmq-stomp
    env_file:
      - ./resources/.env
    ports:
      - "15672:15672"
      - "61613:15674"
      - "5672:5672"
  telegraf:
    image: telegraf
    volumes:
      - ./resources/docker/telegraf.conf:/etc/telegraf/telegraf.conf:rw
    links:
      - influx
      - rabbitmq
  influx:
    image: influxdb
    volumes:
      - ./resources/docker/influx:/var/lib/influxdb
  grafana:
    image: grafana/grafana
    environment:
      - "GF_SERVER_ROOT_URL=http://grafana.server.name"
      - "GF_SECURITY_ADMIN_PASSWORD=queenelizabeth"
    ports:
      - "3000:3000"
    volumes:
      - ./resources/docker/grafana_storage:/var/lib/grafana
    depends_on:
      - influx
  mongodb:
    image: mongo:latest
    expose:
      - 27017
    ports:
      - "27017:27017"

  hadoop-namenode:
    image: bde2020/hadoop-namenode:1.1.0-hadoop2.8-java8
    container_name: hadoop-namenode
    volumes:
      - ./resources/docker/hadoop/data/namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=smug
    env_file:
      - ./resources/docker/hadoop.env
    ports:
      - 50070:50070
  hadoop-datanode:
    image: bde2020/hadoop-datanode:1.1.0-hadoop2.8-java8
    depends_on:
      - hadoop-namenode
    volumes:
      - ./resources/docker/hadoop/data/datanode:/hadoop/dfs/data
    env_file:
      - ./resources/docker/hadoop.env
    ports:
      - 50075:50075
  spark-master:
    build: ./resources/pyspark/.
    command: bin/spark-class org.apache.spark.deploy.master.Master -h spark-master
    hostname: spark-master
    environment:
      MASTER: spark://spark-master:7077
      SPARK_PUBLIC_DNS: localhost
    expose:
      - 8080
      - 7077
    ports:
      - 8080:8080
      - 7077:7077
    env_file:
      - ./resources/docker/hadoop.env
    links:
      - mongodb
    volumes:
      - ./resources/docker/spark:/data
  spark-worker-1:
    build: ./resources/pyspark/.
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    hostname: spark-worker-1
    environment:
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 2g
      SPARK_WORKER_PORT: 8881
      SPARK_WORKER_WEBUI_PORT: 8081
      SPARK_PUBLIC_DNS: localhost
    env_file:
      - ./resources/docker/hadoop.env
    links:
      - spark-master
    volumes:
      - ./resources/docker/spark:/data
    expose:
      - 8881
    ports:
      - 8081:8081
  spark-worker-2:
    build: ./resources/pyspark/.
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    hostname: spark-worker-2
    environment:
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 2g
      SPARK_WORKER_PORT: 8882
      SPARK_WORKER_WEBUI_PORT: 8082
      SPARK_PUBLIC_DNS: localhost
    env_file:
      - ./resources/docker/hadoop.env
    links:
      - spark-master
    volumes:
      - ./resources/docker/spark:/data
    expose:
      - 8882
    ports:
      - 8082:8082
